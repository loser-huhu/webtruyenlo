import os
import json
import time
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ----------------- C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ------------------
# L·∫•y ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c hi·ªán t·∫°i (n∆°i file python n√†y n·∫±m)
current_dir = os.path.dirname(os.path.abspath(__file__))

# ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c Json (n·∫±m c√πng c·∫•p v·ªõi th∆∞ m·ª•c code hi·ªán t·∫°i)
json_dir = os.path.join(current_dir, "..", "Json")

# ƒê·∫£m b·∫£o th∆∞ m·ª•c Json t·ªìn t·∫°i
os.makedirs(json_dir, exist_ok=True)

# ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß cho 2 file json c·∫ßn l∆∞u
raw_next_data_path = os.path.join(json_dir, "raw_next_data.json")
full_data_path = os.path.join(json_dir, "truyenchu_full_data.json")

# -------------------------------------------------------

# C·∫•u h√¨nh tr√¨nh duy·ªát
options = Options()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

# Kh·ªüi t·∫°o driver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# T·∫£i cookie n·∫øu c√≥
try:
    with open("cookies.pkl", "rb") as f:
        cookies = pickle.load(f)
    driver.get("https://truyenchu.com.vn/")
    for cookie in cookies:
        driver.add_cookie(cookie)
except FileNotFoundError:
    print("üîç Kh√¥ng t√¨m th·∫•y cookie, s·∫Ω th·ª≠ ƒëƒÉng nh·∫≠p...")

# Truy c·∫≠p trang ch·ªß
url = "https://truyenchu.com.vn/"
print("üîç ƒêang t·∫£i trang ch·ªß...")
driver.get(url)
time.sleep(3)

# L·∫•y d·ªØ li·ªáu JSON t·ª´ script
try:
    script_tag = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, '//script[@id="__NEXT_DATA__"]'))
    )
    json_data = script_tag.get_attribute("textContent")
except Exception as e:
    print(f"‚ùå L·ªói khi l·∫•y d·ªØ li·ªáu JSON: {e}")
    driver.quit()
    exit()

# Parse JSON
try:
    data = json.loads(json_data)
    page_props = data["props"]["pageProps"]
    
    # L∆∞u d·ªØ li·ªáu th√¥ ƒë·ªÉ debug (l∆∞u to√†n b·ªô d·ªØ li·ªáu g·ªëc)
    with open(raw_next_data_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu th√¥ v√†o {raw_next_data_path}")
    
    # T·∫°o dictionary ƒë·ªÉ l∆∞u tr·ªØ t·∫•t c·∫£ c√°c m·ª•c
    all_data = {}
    
    # Thu th·∫≠p d·ªØ li·ªáu t·ª´ c√°c m·ª•c kh√°c nhau
    if "editorChoice" in page_props:
        editor_choice_books = page_props["editorChoice"]
        all_data["Bi√™n t·∫≠p vi√™n ƒë·ªÅ c·ª≠"] = editor_choice_books[:8]
        all_data["ƒêang ƒë·ªçc"] = editor_choice_books[8:]
    if "lastUpdate" in page_props:
        all_data["M·ªõi c·∫≠p nh·∫≠t"] = page_props["lastUpdate"]
    if "topRead" in page_props:
        all_data["ƒê·ªçc nhi·ªÅu"] = page_props["topRead"]
    if "topPopular" in page_props:
        all_data["Th·ªãnh H√†nh"] = page_props["topPopular"]
    if "topPromote" in page_props:
        all_data["ƒê·ªÅ C·ª≠"] = page_props["topPromote"]
    if "topRate" in page_props:
        all_data["ƒê√°nh gi√° cao"] = page_props["topRate"]
    if "lastComplete" in page_props:
        all_data["M·ªõi ho√†n th√†nh"] = page_props["lastComplete"]
    
    print(f"‚úÖ ƒê√£ t√¨m th·∫•y {len(all_data)} m·ª•c truy·ªán")
    
except Exception as e:
    print(f"‚ùå L·ªói khi parse JSON: {e}")
    driver.quit()
    exit()

# ƒê√≥ng tr√¨nh duy·ªát
driver.quit()

# √Ånh x·∫° th·ªÉ lo·∫°i
genre_map = {
    1: "Ti√™n hi·ªáp", 2: "Huy·ªÅn huy·ªÖn", 3: "Khoa huy·ªÖn", 4: "V√µng du", 5: "ƒê√¥ th·ªã",
    6: "L·ªãch s·ª≠", 7: "Qu√¢n s·ª±", 8: "Trinh th√°m", 9: "Linh d·ªã", 10: "T√¨nh c·∫£m",
    11: "ƒêam m·ªπ", 12: "N·ªØ hi·ªáp", 13: "C·ªï ƒë·∫°i", 14: "Hi·ªán ƒë·∫°i", 15: "Xuy√™n kh√¥ng",
    16: "Tr·ªçng sinh", 17: "H√†i h∆∞·ªõc", 18: "ƒêi·ªÅn vƒÉn", 19: "Cung ƒë·∫•u", 20: "Gia ƒë·∫•u",
    21: "D·ªã gi·ªõi", 22: "D·ªã nƒÉng", 23: "Tr√πng sinh", 24: "Nhan s·∫Øc", 25: "Ng·ªçt s·ªßng",
    26: "C∆∞·ªùng th·ªß h√†o ƒëo·∫°t", 27: "S·ªßng", 28: "Ng∆∞·ª£c", 29: "H·ªá th·ªëng", 30: "Thanh mai tr√∫c m√£",
    31: "M·∫°t th·∫ø", 32: "Truy·ªÅn thuy·∫øt", 33: "V∆∞·ªùn tr∆∞·ªùng", 34: "T√¢y ph∆∞∆°ng", 35: "C·ªï ƒëi·ªÉn",
    36: "Th·∫ßn tho·∫°i", 37: "Qu·ª∑ d·ªã", 38: "T√°i sinh", 39: "Tu ch√¢n", 40: "Ti√™n l·ªØ k·ª≥ duy√™n"
}

# C·∫•u tr√∫c d·ªØ li·ªáu k·∫øt qu·∫£
results = {key: [] for key in all_data.keys()}

# X·ª≠ l√Ω t·ª´ng m·ª•c
for category_name, books in all_data.items():
    print(f"\nüìö ƒêang x·ª≠ l√Ω m·ª•c: {category_name} - S·ªë truy·ªán: {len(books)}")
    
    for idx, book in enumerate(books, 1):
        try:
            slug = book.get("slug", "") or book.get("bookId", "")
            if not slug:
                continue
                
            link = f"https://truyenchu.com.vn/{slug}" if slug else book.get("url", "")
            
            # X·ª≠ l√Ω ·∫£nh b√¨a
            cover_url = book.get("coverUrl", "") or book.get("cover", "") or book.get("image", "")
            if cover_url:
                if not cover_url.startswith("http"):
                    cover_url = f"https://static.truyenchu.com.vn{cover_url}"
            else:
                cover_url = f"https://static.truyenchu.com.vn/Data/{slug}/300.jpg"
            
            # X·ª≠ l√Ω th·ªÉ lo·∫°i
            genre_ids = book.get("genres", []) or book.get("genreIds", [])
            genres = [genre_map.get(id, f"Th·ªÉ lo·∫°i {id}") for id in genre_ids if id in genre_map]
            
            # X·ª≠ l√Ω t√™n truy·ªán
            name = book.get("name", "") or book.get("bookName", "") or book.get("title", "Kh√¥ng r√µ")
            
            # X·ª≠ l√Ω s·ªë ch∆∞∆°ng
            chapter_count = book.get("chapterCount", 0) or book.get("totalChapter", 0) or book.get("chapters", 0)
            
            # Th√™m v√†o k·∫øt qu·∫£
            results[category_name].append({
                "ten": name.strip(),
                "link": link,
                "mo_ta": book.get("introduction", "") or book.get("description", "").strip(),
                "so_chuong": f"{chapter_count} ch∆∞∆°ng",
                "the_loai": ", ".join(genres),
                "anh": cover_url
            })
            
            print(f"‚úÖ [{idx}/{len(books)}] ƒê√£ x·ª≠ l√Ω: {name}")

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω truy·ªán: {name} - {e}")

# L∆∞u k·∫øt qu·∫£ x·ª≠ l√Ω v√†o file ƒë·∫ßy ƒë·ªß
with open(full_data_path, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

# Th·ªëng k√™
total_books = sum(len(books) for books in results.values())
print(f"\n‚úÖ ƒê√£ x·ª≠ l√Ω th√†nh c√¥ng {total_books} truy·ªán")
for category, books in results.items():
    print(f"- {category}: {len(books)} truy·ªán")
print(f"üéâ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {full_data_path}")
